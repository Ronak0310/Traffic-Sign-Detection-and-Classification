{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Importing needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cv2\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Loading *labels*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ClassId              SignName\n",
      "0        0  Speed limit (20km/h)\n",
      "1        1  Speed limit (30km/h)\n",
      "2        2  Speed limit (50km/h)\n",
      "3        3  Speed limit (60km/h)\n",
      "4        4  Speed limit (70km/h)\n",
      "\n",
      "Speed limit (20km/h)\n",
      "Speed limit (30km/h)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.read_csv(r'D:\\Study_3\\GTSRB_new\\signnames.csv')\n",
    "\n",
    "# Check point\n",
    "print(labels.head())\n",
    "print()\n",
    "\n",
    "print(labels.iloc[0][1])  # Speed limit (20km/h)\n",
    "print(labels['SignName'][1]) # Speed limit (30km/h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Loading trained Keras CNN model for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fa0bf55490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\n",
    "model = load_model(r'D:\\Study_3\\GTSRB_new\\training\\new_model.h5')\n",
    "\n",
    "# Loading mean image to use for preprocessing further (mean image substraction)\n",
    "with open(r'D:\\Study_3\\mean_image_gray.pickle', 'rb') as f:\n",
    "    mean = pickle.load(f) \n",
    "    \n",
    "print(mean['mean_image_gray'].shape) \n",
    "plt.imshow(mean['mean_image_gray'].reshape(32, 32, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading YOLO v3 network by OpenCV dnn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading *trained weights* and *cfg file* into the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_weights = r'D:\\Study_3\\weights\\yolov3_ts_train_last.weights'\n",
    "path_to_cfg = r'D:\\Study_3\\yolov3_ts_test.cfg'\n",
    "\n",
    "# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\n",
    "network = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\n",
    "\n",
    "# To use with GPU\n",
    "network.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "network.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting output layers where detections are made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "['yolo_82', 'yolo_94', 'yolo_106']\n"
     ]
    }
   ],
   "source": [
    "# Getting names of all YOLO v3 layers\n",
    "layers_all = network.getLayerNames()\n",
    "\n",
    "# Check point\n",
    "# print(layers_all)\n",
    "\n",
    "# Getting only detection YOLO v3 layers that are 82, 94 and 106\n",
    "layers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
    "\n",
    "# Check point\n",
    "print()\n",
    "print(layers_names_output)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting probability, threshold and colour for bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(43, 3)\n",
      "[ 78 212  19]\n"
     ]
    }
   ],
   "source": [
    "# Minimum probability to eliminate weak detections\n",
    "probability_minimum = 0.2\n",
    "\n",
    "# Setting threshold to filtering weak bounding boxes by non-maximum suppression\n",
    "threshold = 0.2\n",
    "\n",
    "# Generating colours for bounding boxes\n",
    "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "# Check point\n",
    "print(type(colours))  \n",
    "print(colours.shape)\n",
    "print(colours[0])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading video from a file by VideoCapture object of cv2 class\n",
    "video = cv2.VideoCapture(r'C:\\Users\\ronak\\Desktop\\video_for_test\\input_fr.mp4')\n",
    "\n",
    "# Writer that will be used to write processed frames\n",
    "writer = None\n",
    "\n",
    "# Variables for spatial dimensions of the frames\n",
    "h, w = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut frame preprocessing...\n",
    "def grayscale(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "def equalize(img):\n",
    "    img = cv2.equalizeHist(img)\n",
    "    return img\n",
    "\n",
    "def preprocessing(img):\n",
    "    img = grayscale(img)\n",
    "    img = equalize(img)\n",
    "    # img = img/255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing frames in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame number 1 took 0.59359 seconds\n",
      "Frame number 2 took 0.58769 seconds\n",
      "Frame number 3 took 0.58954 seconds\n",
      "Frame number 4 took 0.61808 seconds\n",
      "Frame number 5 took 0.58429 seconds\n",
      "Frame number 6 took 0.58412 seconds\n",
      "Frame number 7 took 0.58314 seconds\n",
      "Frame number 8 took 0.58485 seconds\n",
      "Frame number 9 took 0.58183 seconds\n",
      "Frame number 10 took 0.57891 seconds\n",
      "Frame number 11 took 0.59234 seconds\n",
      "Frame number 12 took 0.58388 seconds\n",
      "Frame number 13 took 0.58161 seconds\n",
      "Frame number 14 took 0.62239 seconds\n",
      "Frame number 15 took 0.58463 seconds\n",
      "Frame number 16 took 0.61265 seconds\n",
      "Frame number 17 took 0.59823 seconds\n",
      "Frame number 18 took 0.62056 seconds\n",
      "Frame number 19 took 0.59286 seconds\n",
      "Frame number 20 took 0.59539 seconds\n",
      "Frame number 21 took 0.57731 seconds\n",
      "Frame number 22 took 0.56943 seconds\n",
      "Frame number 23 took 0.57469 seconds\n",
      "Frame number 24 took 0.58799 seconds\n",
      "Frame number 25 took 0.51744 seconds\n",
      "Frame number 26 took 0.40042 seconds\n",
      "Frame number 27 took 0.40791 seconds\n",
      "Frame number 28 took 0.40237 seconds\n",
      "Frame number 29 took 0.40847 seconds\n",
      "Frame number 30 took 0.39119 seconds\n",
      "Frame number 31 took 0.41977 seconds\n",
      "Frame number 32 took 0.39454 seconds\n",
      "Frame number 33 took 0.41309 seconds\n",
      "Frame number 34 took 0.39176 seconds\n",
      "Frame number 35 took 0.38902 seconds\n",
      "Frame number 36 took 0.39822 seconds\n",
      "Frame number 37 took 0.39829 seconds\n",
      "Frame number 38 took 0.41022 seconds\n",
      "Frame number 39 took 0.40253 seconds\n",
      "Frame number 40 took 0.40121 seconds\n",
      "Frame number 41 took 0.40090 seconds\n",
      "Frame number 42 took 0.41604 seconds\n",
      "Frame number 43 took 0.47290 seconds\n",
      "Frame number 44 took 0.57968 seconds\n",
      "Frame number 45 took 0.58002 seconds\n",
      "Frame number 46 took 0.58416 seconds\n",
      "Frame number 47 took 0.56995 seconds\n",
      "Frame number 48 took 0.58410 seconds\n",
      "Frame number 49 took 0.58902 seconds\n",
      "Frame number 50 took 0.58164 seconds\n",
      "Frame number 51 took 0.58468 seconds\n",
      "Frame number 52 took 0.57620 seconds\n",
      "Frame number 53 took 0.64839 seconds\n",
      "Frame number 54 took 0.82712 seconds\n",
      "Frame number 55 took 0.52875 seconds\n",
      "Frame number 56 took 0.57028 seconds\n",
      "Frame number 57 took 0.70819 seconds\n",
      "Frame number 58 took 1.05633 seconds\n",
      "Frame number 59 took 1.70531 seconds\n",
      "Frame number 60 took 1.07366 seconds\n",
      "Frame number 61 took 0.84984 seconds\n",
      "Frame number 62 took 0.74778 seconds\n",
      "Frame number 63 took 0.76874 seconds\n",
      "Frame number 64 took 0.65101 seconds\n",
      "Frame number 65 took 0.76390 seconds\n",
      "Frame number 66 took 0.73320 seconds\n",
      "Frame number 67 took 0.58461 seconds\n",
      "Frame number 68 took 0.69061 seconds\n",
      "Frame number 69 took 0.70133 seconds\n",
      "Frame number 70 took 0.68858 seconds\n",
      "Frame number 71 took 0.76864 seconds\n",
      "Frame number 72 took 0.71298 seconds\n",
      "Frame number 73 took 0.74981 seconds\n",
      "Frame number 74 took 0.90116 seconds\n",
      "Frame number 75 took 0.83664 seconds\n",
      "Frame number 76 took 0.73858 seconds\n",
      "Frame number 77 took 0.77543 seconds\n",
      "Frame number 78 took 0.74996 seconds\n",
      "Frame number 79 took 0.71801 seconds\n",
      "Frame number 80 took 0.92736 seconds\n",
      "Frame number 81 took 0.86004 seconds\n",
      "Frame number 82 took 0.75321 seconds\n",
      "Frame number 83 took 0.82273 seconds\n",
      "Frame number 84 took 1.05764 seconds\n",
      "Frame number 85 took 0.91360 seconds\n",
      "Frame number 86 took 0.85345 seconds\n",
      "Frame number 87 took 0.88151 seconds\n",
      "Frame number 88 took 0.84256 seconds\n",
      "Frame number 89 took 1.04422 seconds\n",
      "Frame number 90 took 0.90264 seconds\n",
      "Frame number 91 took 0.85686 seconds\n",
      "Frame number 92 took 0.90510 seconds\n",
      "Frame number 93 took 1.28365 seconds\n",
      "Frame number 94 took 0.66657 seconds\n",
      "Frame number 95 took 0.72772 seconds\n",
      "Frame number 96 took 0.80393 seconds\n",
      "Frame number 97 took 0.75851 seconds\n",
      "Frame number 98 took 0.83398 seconds\n",
      "Frame number 99 took 1.06600 seconds\n",
      "Frame number 100 took 0.84520 seconds\n",
      "Frame number 101 took 0.86959 seconds\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Setting default size of plots\n",
    "plt.rcParams['figure.figsize'] = (3, 3)\n",
    "\n",
    "# Variable for counting total amount of frames\n",
    "f = 0\n",
    "\n",
    "# Variable for counting total processing time\n",
    "t = 0\n",
    "\n",
    "# Catching frames in the loop\n",
    "while True:\n",
    "    # Capturing frames one-by-one\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    # If the frame was not retrieved\n",
    "    if not ret:\n",
    "        break\n",
    "       \n",
    "    # Getting spatial dimensions of the frame for the first time\n",
    "    if w is None or h is None:\n",
    "        # Slicing two elements from tuple\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "    # Blob from current frame\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    # Forward pass with blob through output layers\n",
    "    network.setInput(blob)\n",
    "    start = time.time()\n",
    "    output_from_network = network.forward(layers_names_output)\n",
    "    end = time.time()\n",
    "\n",
    "    # Increasing counters\n",
    "    f += 1\n",
    "    t += end - start\n",
    "\n",
    "    # Spent time for current frame\n",
    "    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n",
    "\n",
    "    # Lists for detected bounding boxes, confidences and class's number\n",
    "    bounding_boxes = []\n",
    "    confidences = []\n",
    "    class_numbers = []\n",
    "\n",
    "    # Going through all output layers after feed forward pass\n",
    "    for result in output_from_network:\n",
    "        # Going through all detections from current output layer\n",
    "        for detected_objects in result:\n",
    "            # Getting 80 classes' probabilities for current detected object\n",
    "            scores = detected_objects[5:]\n",
    "            # Getting index of the class with the maximum value of probability\n",
    "            class_current = np.argmax(scores)\n",
    "            # Getting value of probability for defined class\n",
    "            confidence_current = scores[class_current]\n",
    "\n",
    "            # Eliminating weak predictions by minimum probability\n",
    "            if confidence_current > probability_minimum:\n",
    "                # Scaling bounding box coordinates to the initial frame size\n",
    "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                # Getting top left corner coordinates\n",
    "                x_center, y_center, box_width, box_height = box_current\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(class_current)\n",
    "                \n",
    "\n",
    "    # Implementing non-maximum suppression of given bounding boxes\n",
    "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n",
    "\n",
    "    # Checking if there is any detected object been left\n",
    "    if len(results) > 0:\n",
    "        # Going through indexes of results\n",
    "        for i in results.flatten():\n",
    "            # Bounding box coordinates, its width and height\n",
    "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "            \n",
    "            \n",
    "            # Cut fragment with Traffic Sign\n",
    "            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n",
    "\n",
    "            \n",
    "            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n",
    "                pass\n",
    "            else:\n",
    "                c_ts = preprocessing(c_ts)\n",
    "                # Getting preprocessed blob with Traffic Sign of needed shape\n",
    "                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 / 255.0, size=(32, 32), swapRB=True, crop=False)\n",
    "                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_gray']\n",
    "                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n",
    "\n",
    "                # Feeding to the Keras CNN model to get predicted label among 43 classes\n",
    "                scores = model.predict(blob_ts)\n",
    "\n",
    "                # Scores is given for image with 43 numbers of predictions for each class\n",
    "                # Getting only one class with maximum value\n",
    "                prediction = np.argmax(scores)\n",
    "                # print(labels['SignName'][prediction])\n",
    "\n",
    "\n",
    "                # Colour for current bounding box\n",
    "                colour_box_current = colours[class_numbers[i]].tolist()\n",
    "\n",
    "                # Drawing bounding box on the original current frame\n",
    "                cv2.rectangle(frame, (x_min, y_min),\n",
    "                              (x_min + box_width, y_min + box_height),\n",
    "                              colour_box_current, 2)\n",
    "\n",
    "                # Preparing text with label and confidence for current bounding box\n",
    "                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n",
    "                                                       confidences[i])\n",
    "\n",
    "                # Putting text with label and confidence on the original image\n",
    "                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n",
    "\n",
    "\n",
    "    # Initializing writer only once\n",
    "    if writer is None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "        # Writing current processed frame into the video file\n",
    "        writer = cv2.VideoWriter(r'C:\\Users\\ronak\\Desktop\\video_for_test\\fr_result.mp4', fourcc, 25,\n",
    "                                 (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "    # Write processed current frame to the file\n",
    "    writer.write(frame)\n",
    "\n",
    "\n",
    "# Releasing video reader and writer\n",
    "video.release()\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FPS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames 101\n",
      "Total amount of time 67.97068 seconds\n",
      "FPS: 1.5\n"
     ]
    }
   ],
   "source": [
    "print('Total number of frames', f)\n",
    "print('Total amount of time {:.5f} seconds'.format(t))\n",
    "print('FPS:', round((f / t), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='C:\\Users\\ronak\\Desktop\\video_for_test\\fr_result.mp4' target='_blank'>C:\\Users\\ronak\\Desktop\\video_for_test\\fr_result.mp4</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\ronak\\Desktop\\video_for_test\\fr_result.mp4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving locally without committing\n",
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink(r'C:\\Users\\ronak\\Desktop\\video_for_test\\fr_result.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(r\"C:\\Users\\ronak\\Desktop\\video_for_test\\fr_result.mp4\", width=1000, height=576, embed=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
